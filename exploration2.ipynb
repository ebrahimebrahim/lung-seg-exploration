{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9caddba5",
   "metadata": {},
   "source": [
    "*Attention U-Net Based Adversarial Architectures for Chest X-ray Lung Segmentation*\\\n",
    "2020 Gaal, Maga, Lukacs\\\n",
    "[link](https://arxiv.org/pdf/2003.10304.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f933a5",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "## Some publicly available datasets\n",
    "\n",
    "- [JSRT](https://www.ajronline.org/doi/pdf/10.2214/ajr.174.1.1740071)\n",
    "\t- 247 chest X-rays, 154 have lung nodules. Has lung and heart seg.\n",
    "\t- [Get here](http://db.jsrt.or.jp/eng.php) (register at bottom of page)\n",
    "- [Montgomery and Shenzhen](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/)\n",
    "\t- Montgomery contains 138 chest X-rays, 80 healthy, 58 tuberculosis. Has lung seg.\n",
    "\t- Shenzhen contains 662 chest X-rays, 326 healthy, 336 tuberculosis. Has lung seg.\n",
    "\t- [Get both here](https://openi.nlm.nih.gov/faq?it=xg#collection). Look for \"tuberculosis collection\"\n",
    "\n",
    "That’s 1047 images with lung segmentation label. There are larger datasets that have no segmentation label:\n",
    "\n",
    "- [NIH ChestX-ray8](https://arxiv.org/abs/1705.02315)\n",
    "\t- 108,948 CXRs of 32,717 patients with eight text-mined disease labels\n",
    "\t- [this might be a way to download](https://nihcc.app.box.com/v/ChestXray-NIHCC)\n",
    "- [NLST](https://www.nejm.org/doi/10.1056/NEJMoa1102873)\n",
    "\t- There's [this link](https://cdas.cancer.gov/publications/320/), which eventually leads [here](https://cdas.cancer.gov/datasets/nlst/), but I don't see any actual CXR images being made available.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8e455",
   "metadata": {},
   "source": [
    "### JSRT\n",
    "\n",
    "See [this guide](JSRT_UsersGuide.pdf) to the data for details.\n",
    "\n",
    "- `JPCLN***.IMG` for chest lung nodule images, and `JPCNN***.IMG` for non-nodule images. These are important classes to keep in mind for the purpose of proportional train/val/test split.\n",
    "-  coordinates of the upper left of the image are `(0,0)`\n",
    "- Image type: 16-bit Unsigned\n",
    "- Width: 2048 pixels\n",
    "- Height: 2048 pixels\n",
    "- Offset to First Image: 0 bytes\n",
    "- Gap Between Images: 0 bytes\n",
    "\n",
    "You can load the images using [ImageJ](https://imagej.nih.gov/ij/).\n",
    "Just import as \"RAW\" and put in the settings specified by the JSRT guide.\n",
    "\n",
    "Hmm, stuck on this for now... also where are the segmentations? When I load RAW data into Image J I only see an xray image, and I see no reference to segmentation in the JSRT guide. The JSRT download page doesn't say anything about segmentation labels either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f0734",
   "metadata": {},
   "source": [
    "### Shenzhen\n",
    "\n",
    "[The readme](NLM-ChinaCXRSet-ReadMe.pdf).\n",
    "\n",
    "- 336 cases with manifestation of tuberculosis, and \n",
    "- 326 normal cases.\n",
    "\n",
    "- Format: PNG\n",
    "- Image size varies for each X-ray. It is approximately 3K x 3K.\n",
    "\n",
    "- Image file names are coded as `CHNCXR_#####_0/1.png`, where ‘0’ represents the normal and ‘1’\n",
    "represents the abnormal lung. \n",
    "\n",
    "Hmm I also cannot find any lung segmentation for this. And [this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/) that introduces the dataset seems to suggest that it's just the Montgomery set that has the segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6d7e7",
   "metadata": {},
   "source": [
    "### Montgomery\n",
    "\n",
    "[The readme](NLM-MontgomeryCXRSet-ReadMe.pdf).\n",
    "\n",
    "- 58 cases\twith\tmanifestation\tof\ttuberculosis,\tand\t 80 normal\tcases.\n",
    "- Image\t file\t names\tare\t coded\tas\t`MCUCXR_#####_0/1.png`, where\t‘0’\t represents\t the\t normal\tand\t‘1’ represents\tthe\tabnormal\tlung. These are important classes to keep in mind for the purpose of proportional train/val/test split.\n",
    "\n",
    "---\n",
    "\n",
    "- Format:\tPNG\n",
    "- Matrix\tsize\tis\t4020\tx\t4892,\tor\t4892\tx\t4020.\n",
    "- The\tpixel\tspacing\tin\tvertical\tand\thorizontal\tdirections\tis\t0.0875\tmm.\t\n",
    "- Number\tof\tgray\tlevels\tis\t12 bits.\n",
    "\n",
    "---\n",
    "\n",
    "Segmentation:\n",
    "> We\tmanually\tgenerated\tthe\t“gold\tstandard” segmentations\tfor\tthe\tchest\tX-ray\tunder\tthe\tsupervision\tof a\tradiologist.\tWe\tused\tthe\tfollowing\tconventions\tfor outlining\tthe\tlung\tboundaries:\tBoth\tposterior\tand\tanterior\tribs\tare\treadily\tvisible\tin\tthe\tCXRs;\tthe\tpart\tof\tthe\tlung\tbehind\tthe\theart\tis\texcluded.\tWe\tfollow\tanatomical\t landmarks\t such\t as\t the\t boundary\t of\t the\t heart,\t aortic\t arc/line,\t and\t pericardium\t line;\t and\tsharp\tcostophrenic\tangle\tthat\tfollow\tthe\tdiaphragm\tboundary. We\tdraw\tan\tinferred\tboundary\twhen\tthe\tpathology\tis\tsevere\tand\taffects\tthe\tmorphological\tappearance\tof\tthe\tlungs. The\tlung\tboundaries\t(left\tand\tright)\tare\tin\tbinary\timage\tformat\tand\thave\tthe\tsame\tfile\tname\tas\tchest\tXrays\t( e.g.\t`…/left/MCUCXR_#####_0/1.png` or\t`…/right/MCUCXR_#####_0/1.png`). \n",
    "\n",
    "Looks like this is the one we can actually use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d18e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "dataset_base_path = '/home/ebrahim/data/chest_xrays/MontgomerySet'\n",
    "dataset_imgs_path = os.path.join(dataset_base_path, 'CXR_png/')\n",
    "dataset_seg_path_left = os.path.join(dataset_base_path, 'ManualMask/leftMask')\n",
    "dataset_seg_path_right = os.path.join(dataset_base_path, 'ManualMask/rightMask')\n",
    "\n",
    "imgs = glob.glob(os.path.join(dataset_imgs_path, '*.png'))\n",
    "segs_left = glob.glob(os.path.join(dataset_seg_path_left, '*.png'))\n",
    "segs_right = glob.glob(os.path.join(dataset_seg_path_right, '*.png'))\n",
    "\n",
    "file_path_to_ID = lambda p : os.path.basename(p)[7:11]\n",
    "file_path_to_abnormality = lambda p : bool(int(os.path.basename(p)[12]))\n",
    "\n",
    "img_ids = list(map(file_path_to_ID,imgs));\n",
    "seg_ids_left = list(map(file_path_to_ID,segs_left));\n",
    "seg_ids_right = list(map(file_path_to_ID,segs_right));\n",
    "\n",
    "data = []\n",
    "for img in imgs:\n",
    "    img_id = file_path_to_ID(img)\n",
    "    seg_left = segs_left[seg_ids_left.index(img_id)]\n",
    "    seg_right = segs_right[seg_ids_right.index(img_id)]\n",
    "    tuberculosis = file_path_to_abnormality(img)\n",
    "    data.append({\n",
    "        'img' : img,\n",
    "        'seg_left' : seg_left,\n",
    "        'seg_right' : seg_right,\n",
    "        'tuberculosis' : tuberculosis\n",
    "\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9face75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "monai.utils.misc.set_determinism(seed=9274)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4899a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = monai.data.utils.partition_dataset_classes(\n",
    "    data,\n",
    "    classes = list(map(lambda d : d['tuberculosis'], data)),\n",
    "    ratios = (8,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e919f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transforms\n",
    "from typing import Mapping, Hashable, List\n",
    "\n",
    "class MasksToOneHotD(monai.transforms.MapTransform):\n",
    "    def __init__(self, keys: monai.config.KeysCollection,\n",
    "                 keyList: List[Hashable], newKeyName: str) -> None:\n",
    "        super().__init__(keys)\n",
    "        self.keyList = keyList\n",
    "        self.newKeyName = newKeyName\n",
    "        assert(len(keyList)>0)\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Mapping[Hashable, np.ndarray]:\n",
    "        \n",
    "        # (if this were to be contributed, I'd have to pay attention to whether keys are in data)\n",
    "        # (also I'd want to raise more clear exceptions than these asserts)\n",
    "        \n",
    "        assert(all(key in self.keys for key in self.keyList))\n",
    "        assert(all(key in data.keys() for key in self.keyList))\n",
    "        assert(self.newKeyName not in data.keys())\n",
    "        \n",
    "        background_mask = (sum(data[key] for key in self.keyList)==0).astype('int8')\n",
    "        \n",
    "        # Assumes these were numpy arrays.\n",
    "        # If they were torch tensors we'd have to do \"torch.stack\" and use argument \"dim\" instead of \"axis\"\n",
    "        data[self.newKeyName] = np.stack(\n",
    "            [background_mask] + [data[key] for key in self.keyList],\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22032a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "transform_valid = monai.transforms.Compose([\n",
    "    monai.transforms.LoadImageD(keys = ['img']),\n",
    "    monai.transforms.LoadImageD(keys = ['seg_left', 'seg_right'], dtype=\"int8\"),\n",
    "    monai.transforms.TransposeD(keys = ['img', 'seg_left', 'seg_right'], indices = (1,0)),\n",
    "    monai.transforms.AddChannelD(keys = ['img']),\n",
    "    MasksToOneHotD(\n",
    "        keys = ['seg_left', 'seg_right'],\n",
    "        keyList = ['seg_left', 'seg_right'],\n",
    "        newKeyName = 'seg'\n",
    "    ),\n",
    "    monai.transforms.DeleteItemsD(keys = ['seg_left', 'seg_right']),\n",
    "    monai.transforms.ResizeD(\n",
    "        keys = ['img', 'seg'],\n",
    "        spatial_size=(image_size,image_size),\n",
    "        mode = ['bilinear', 'nearest']\n",
    "    ),\n",
    "    monai.transforms.ToTensorD(keys = ['img', 'seg']),\n",
    "])\n",
    "\n",
    "transform_train = monai.transforms.Compose([\n",
    "    transform_valid,\n",
    "    monai.transforms.RandZoomD(\n",
    "        keys = ['img', 'seg'],\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        prob=1.,\n",
    "        padding_mode=\"constant\",\n",
    "        min_zoom = 0.7,\n",
    "        max_zoom=1.3,\n",
    "    ),\n",
    "    monai.transforms.RandRotateD(\n",
    "        keys = ['img', 'seg'],\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        prob=1.,\n",
    "        range_x = np.pi/8, # TODO decrease ?\n",
    "        padding_mode=\"zeros\",\n",
    "    ),\n",
    "    \n",
    "    monai.transforms.RandGaussianSmoothD(\n",
    "        keys = ['img'],\n",
    "        prob = 0.4\n",
    "    ),\n",
    "    monai.transforms.RandAdjustContrastD(\n",
    "        keys = ['img'],\n",
    "        prob=0.4,\n",
    "    ),\n",
    "    monai.transforms.ToNumpyD(keys=['img']),\n",
    "    monai.transforms.RandHistogramShiftD(\n",
    "        keys = ['img'],\n",
    "        prob=0.2,\n",
    "    ),\n",
    "    monai.transforms.Rand2DElasticD(\n",
    "        keys = ['img', 'seg'],\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        prob=0.2,\n",
    "        spacing = 100,\n",
    "        magnitude_range = [0,15]\n",
    "    ),\n",
    "    monai.transforms.ToTensorD(keys=['img', 'seg']),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = monai.data.CacheDataset(data_train, transform_train)\n",
    "dataset_valid = monai.data.CacheDataset(data_valid, transform_valid)\n",
    "# dataset_train = monai.data.Dataset(data_train, transform_train)\n",
    "# dataset_valid = monai.data.Dataset(data_valid, transform_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a917619",
   "metadata": {},
   "source": [
    "# Previewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da01829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(data_item):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    im = data_item['img'].expand((3,)+data_item['img'].shape[1:])\n",
    "    im = im/im.max()\n",
    "    seg = data_item['seg'].float()\n",
    "    im[1,:,:] *= 1-0.3*seg[1,:,:]\n",
    "    im[2,:,:] *= 1-0.3*seg[2,:,:]\n",
    "    im = np.transpose(im,axes=(1,2,0))\n",
    "    plt.imshow(im, cmap='bone')\n",
    "    plt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae5319",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "preview(random.choice(dataset_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74b25d",
   "metadata": {},
   "source": [
    "Note that some images have different original dimensions. Some are (4020, 4892) and some are (4892, 4020). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fa90b",
   "metadata": {},
   "source": [
    "# seg net\n",
    "\n",
    "Structure of U-Net is inspired by this paper: https://arxiv.org/abs/1703.08770\n",
    "\n",
    "But it's not exactly the same.\n",
    "\n",
    "And it looks like there's one giant deconvolution step at the end, instead of having a symmetric looking unet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cc865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spatial_dims = 2;\n",
    "image_channels = 1;\n",
    "seg_channels = 3; # left lung, right lung, background\n",
    "seg_net_channel_seq = (8,8,16,32,64,64)\n",
    "stride_seq = (2,2,2,2,2) # I don't know why, but MONAI unet insists on this being one shorter than I expect,\n",
    "# and then it forces a stride of 1 at that last step.\n",
    "dropout_seg_net = 0.5\n",
    "num_res_units = 1\n",
    "\n",
    "seg_net = monai.networks.nets.UNet(\n",
    "    spatial_dims = spatial_dims,\n",
    "    in_channels = image_channels,\n",
    "    out_channels = seg_channels, \n",
    "    channels = seg_net_channel_seq,\n",
    "    strides = (2,2,2,2,2),\n",
    "    dropout = dropout_seg_net,\n",
    "    num_res_units = num_res_units\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in seg_net.parameters())\n",
    "print(f\"seg_net has {num_params} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9f0e9",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = monai.losses.DiceLoss(\n",
    "    to_onehot_y = False, # the segs we pass in are already in one-hot form due to MasksToOneHotD defined above\n",
    "    softmax = True, # Note that our segmentation network is missing the softmax at the end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8553e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test drive\n",
    "data_item = dataset_train[42]\n",
    "seg_pred = seg_net(data_item['img'].unsqueeze(0)) # shape is (1,3,1024,1024), which is (B,N,H,W)\n",
    "\n",
    "dice_loss(\n",
    "    seg_net(data_item['img'].unsqueeze(0)), # input, one-hot\n",
    "    data_item['seg'].unsqueeze(0), # target, one-hot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a401c0",
   "metadata": {},
   "source": [
    "# Previewing seg net outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddaac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_seg_net(data_item, figsize=(15,10), print_loss = True):\n",
    "    \n",
    "    seg_net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        im_device = data_item['img'].to(next(seg_net.parameters()).device.type)\n",
    "        seg_pred = seg_net(im_device.unsqueeze(0))[0].cpu()\n",
    "        _, max_indices = seg_pred.max(dim=0)\n",
    "        seg_pred_mask1 = (max_indices==1).type(torch.int8)\n",
    "        seg_pred_mask2 = (max_indices==2).type(torch.int8)\n",
    "\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "        im = data_item['img'].expand((3,)+data_item['img'].shape[1:])\n",
    "        im = im/im.max()\n",
    "\n",
    "        seg_true = data_item['seg'].float()\n",
    "        im_true = im.clone()\n",
    "        im_true[1,:,:] *= 1-0.3*seg_true[1,:,:]\n",
    "        im_true[2,:,:] *= 1-0.3*seg_true[2,:,:]\n",
    "        im_true = np.transpose(im_true,axes=(1,2,0))\n",
    "        ax1.imshow(im_true, cmap='bone')\n",
    "        ax1.set_title(\"true seg overlay\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2.imshow(max_indices)\n",
    "        ax2.set_title(\"predicted seg\")\n",
    "        ax2.axis('off')\n",
    "\n",
    "        im_pred = im.clone()\n",
    "        im_pred[1,:,:] *= 1-0.6*seg_pred_mask1\n",
    "        im_pred[2,:,:] *= 1-0.6*seg_pred_mask2\n",
    "        im_pred = np.transpose(im_pred,axes=(1,2,0))\n",
    "        ax3.imshow(im_pred, cmap='bone')\n",
    "        ax3.set_title(\"predicted seg overlay\")\n",
    "        ax3.axis('off')\n",
    "\n",
    "        plt.show();\n",
    "\n",
    "        if print_loss:\n",
    "            loss = dice_loss(\n",
    "                seg_pred.unsqueeze(0),\n",
    "                data_item['seg'].unsqueeze(0),\n",
    "            )\n",
    "            print(f\"Dice loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c3c02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try seg_net on a random image.\n",
    "preview_seg_net(random.choice(dataset_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50858059",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0]['seg'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d7c51",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757d0a9",
   "metadata": {},
   "source": [
    "## Pretraining seg net alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57875d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seg_net.to('cuda')\n",
    "\n",
    "dataloader_train = monai.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = monai.data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "\n",
    "max_epochs = 100\n",
    "training_losses = [] \n",
    "validation_losses = []\n",
    "\n",
    "for epoch_number in range(max_epochs):\n",
    "    \n",
    "    print(f\"Epoch {epoch_number+1}/{max_epochs} ...\")\n",
    "    \n",
    "    if (epoch_number%5==0):\n",
    "        preview_seg_net(dataset_valid[18], figsize=(6,6), print_loss=False);\n",
    "    \n",
    "    seg_net.train()\n",
    "    losses = []\n",
    "    for batch in dataloader_train:\n",
    "        imgs = batch['img'].to('cuda')\n",
    "        true_segs = batch['seg'].to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_segs = seg_net(imgs)\n",
    "        loss = dice_loss(predicted_segs, true_segs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    training_loss = np.mean(losses)\n",
    "    training_losses.append([epoch_number, training_loss])\n",
    "    \n",
    "    print(f\"\\ttraining loss: {training_loss}\")\n",
    "\n",
    "    if (epoch_number%5==0):\n",
    "    \n",
    "        seg_net.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_valid:\n",
    "                imgs = batch['img'].to('cuda')\n",
    "                true_segs = batch['seg'].to('cuda')\n",
    "                predicted_segs = seg_net(imgs)\n",
    "                loss = dice_loss(predicted_segs, true_segs)\n",
    "                losses.append(loss.item())\n",
    "            validation_loss = np.mean(losses)\n",
    "\n",
    "        print(f\"\\tvalidation loss: {validation_loss}\")\n",
    "        \n",
    "        validation_losses.append([epoch_number, validation_loss])\n",
    "\n",
    "del imgs, true_segs, predicted_segs, loss\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c948d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Try on a random validation image\n",
    "data_item_index = random.choice(range(len(dataset_valid)))\n",
    "print(data_item_index)\n",
    "data_item = dataset_valid[data_item_index]\n",
    "with torch.no_grad():\n",
    "    preview_seg_net(data_item);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '0008'\n",
    "if (os.path.exists(f'seg_net{run_id}.pth')):\n",
    "    del run_id\n",
    "    raise Exception(\"Please change run_id so you don't overwrite things.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "# torch.save(seg_net.state_dict(),f'seg_net{run_id}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ed856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "# seg_net.load_state_dict(torch.load(f'seg_net{run_id}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_against_epoch_numbers(epoch_value_pairs, label):\n",
    "    array = np.array(epoch_value_pairs)\n",
    "    plt.plot(array[:,0], array[:,1], label=label)\n",
    "\n",
    "plot_against_epoch_numbers(training_losses, label=\"training\")\n",
    "plot_against_epoch_numbers(validation_losses, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('dice loss')\n",
    "plt.title('seg net training')\n",
    "plt.savefig(f'seg_net_losses{run_id}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0985a1",
   "metadata": {},
   "source": [
    "# Continue training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aec237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to continue training without clearing out the losses graph or the above cell output\n",
    "# It's just a copy of the training cell above\n",
    "# (I should really encapsulate the training loop...)\n",
    "\n",
    "# ok i did lower the learning rate\n",
    "\n",
    "seg_net.to('cuda')\n",
    "\n",
    "dataloader_train = monai.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=32,\n",
    "    num_workers=8,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloader_valid = monai.data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "\n",
    "additional_epochs = 200\n",
    "\n",
    "for epoch_number in range(max_epochs,max_epochs + additional_epochs):\n",
    "    \n",
    "    print(f\"Epoch {epoch_number+1}/{max_epochs} ...\")\n",
    "    \n",
    "    if (epoch_number%5==0):\n",
    "        preview_seg_net(dataset_valid[18], figsize=(6,6), print_loss=False);\n",
    "    \n",
    "    seg_net.train()\n",
    "    losses = []\n",
    "    for batch in dataloader_train:\n",
    "        imgs = batch['img'].to('cuda')\n",
    "        true_segs = batch['seg'].to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_segs = seg_net(imgs)\n",
    "        loss = dice_loss(predicted_segs, true_segs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    training_loss = np.mean(losses)\n",
    "    training_losses.append([epoch_number, training_loss])\n",
    "    \n",
    "    print(f\"\\ttraining loss: {training_loss}\")\n",
    "\n",
    "    if (epoch_number%5==0):\n",
    "    \n",
    "        seg_net.eval()\n",
    "        losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_valid:\n",
    "                imgs = batch['img'].to('cuda')\n",
    "                true_segs = batch['seg'].to('cuda')\n",
    "                predicted_segs = seg_net(imgs)\n",
    "                loss = dice_loss(predicted_segs, true_segs)\n",
    "                losses.append(loss.item())\n",
    "            validation_loss = np.mean(losses)\n",
    "\n",
    "        print(f\"\\tvalidation loss: {validation_loss}\")\n",
    "        \n",
    "        validation_losses.append([epoch_number, validation_loss])\n",
    "\n",
    "del imgs, true_segs, predicted_segs, loss\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
