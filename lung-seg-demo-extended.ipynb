{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9caddba5",
   "metadata": {},
   "source": [
    "This differs from lung-seg-demo in that some additional MONAI features are showcased. The following additonal features are shown here that are not in lung-seg-demo:\n",
    "- PersistentDataset (seen [here](#Preparing-Data-Lists))\n",
    "- Sliding window inference (seen [here](#Sliding-Window-Inference))\n",
    "    - Also, a transfrom `RandomSpatialCropD` is introduced into the training, to help train the network on image patches. (See that [here](#Creating-Transforms).)\n",
    "- Metrics (dice metric, seen [here](#Defining-a-metric))\n",
    "- Engines and event handlers (seen in the alternative approach to training [here](#Engines-appraoch-to-training))\n",
    "    - Also, automatic mixed precision (AMP) is seen here.\n",
    "\n",
    "# Table of Contents\n",
    "* [Montgomery and Shenzhen Datasets](#Montgomery-and-Shenzhen-Datasets)\n",
    "* [Preparing Data Lists](#Preparing-Data-Lists)\n",
    "* [Creating Transforms](#Creating-Transforms)\n",
    "* [PersistentDatasets](#PersistentDatasets)\n",
    "* [Previewing](#Previewing)\n",
    "* [Define the Segmentation Network](#Define-the-Segmentation-Network)\n",
    "* [Defining the Loss Function](#Defining-the-Loss-Function)\n",
    "* [Defining a metric](#Defining-a-metric)\n",
    "* [Previewing Segmentation Network Outputs](#Previewing-Segmentation-Network-Outputs)\n",
    "* [Training](#Training)\n",
    "* [Saving](#Saving)\n",
    "* [Plotting](#Plotting)\n",
    "* [Inference](#Inference)\n",
    "\t* [Sliding Window Inference](#Sliding-Window-Inference)\n",
    "* [Engines appraoch to training](#Engines-appraoch-to-training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f933a5",
   "metadata": {},
   "source": [
    "# Montgomery and Shenzhen Datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- [Article about both datasets](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/)\n",
    "- Montgomery contains 138 chest X-rays, 80 healthy, 58 tuberculosis. Has lung seg.\n",
    "- Shenzhen contains 662 chest X-rays, 326 healthy, 336 tuberculosis. Has lung seg.\n",
    "- [Get both here](https://openi.nlm.nih.gov/faq?it=xg#collection). Look for \"tuberculosis collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f0734",
   "metadata": {},
   "source": [
    "### Shenzhen\n",
    "\n",
    "[The readme](NLM-ChinaCXRSet-ReadMe.pdf).\n",
    "\n",
    "- 336 cases with manifestation of tuberculosis, and \n",
    "- 326 normal cases.\n",
    "\n",
    "- Format: PNG\n",
    "- Image size varies for each X-ray. It is approximately 3K x 3K.\n",
    "\n",
    "- Image file names are coded as `CHNCXR_#####_0/1.png`, where ‘0’ represents the normal and ‘1’\n",
    "represents the abnormal lung. \n",
    "\n",
    "Segmentation can be obtained separately [here](https://www.kaggle.com/yoctoman/shcxr-lung-mask), and it was done manually by: \"students and teachers of Computer Engineering Department, Faculty of Informatics and Computer Engineering, National Technical University of Ukraine \"Igor Sikorsky Kyiv Polytechnic Institute\", Kyiv, Ukraine.\" So, not necessarily medical experts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6d7e7",
   "metadata": {},
   "source": [
    "### Montgomery\n",
    "\n",
    "[The readme](NLM-MontgomeryCXRSet-ReadMe.pdf).\n",
    "\n",
    "- 58 cases\twith\tmanifestation\tof\ttuberculosis,\tand\t 80 normal\tcases.\n",
    "- Image\t file\t names\tare\t coded\tas\t`MCUCXR_#####_0/1.png`, where\t‘0’\t represents\t the\t normal\tand\t‘1’ represents\tthe\tabnormal\tlung. These are important classes to keep in mind for the purpose of proportional train/val/test split.\n",
    "\n",
    "---\n",
    "\n",
    "- Format:\tPNG\n",
    "- Matrix\tsize\tis\t4020\tx\t4892,\tor\t4892\tx\t4020.\n",
    "- The\tpixel\tspacing\tin\tvertical\tand\thorizontal\tdirections\tis\t0.0875\tmm.\t\n",
    "- Number\tof\tgray\tlevels\tis\t12 bits.\n",
    "\n",
    "---\n",
    "\n",
    "Segmentation:\n",
    "> We\tmanually\tgenerated\tthe\t“gold\tstandard” segmentations\tfor\tthe\tchest\tX-ray\tunder\tthe\tsupervision\tof a\tradiologist.\tWe\tused\tthe\tfollowing\tconventions\tfor outlining\tthe\tlung\tboundaries:\tBoth\tposterior\tand\tanterior\tribs\tare\treadily\tvisible\tin\tthe\tCXRs;\tthe\tpart\tof\tthe\tlung\tbehind\tthe\theart\tis\texcluded.\tWe\tfollow\tanatomical\t landmarks\t such\t as\t the\t boundary\t of\t the\t heart,\t aortic\t arc/line,\t and\t pericardium\t line;\t and\tsharp\tcostophrenic\tangle\tthat\tfollow\tthe\tdiaphragm\tboundary. We\tdraw\tan\tinferred\tboundary\twhen\tthe\tpathology\tis\tsevere\tand\taffects\tthe\tmorphological\tappearance\tof\tthe\tlungs. The\tlung\tboundaries\t(left\tand\tright)\tare\tin\tbinary\timage\tformat\tand\thave\tthe\tsame\tfile\tname\tas\tchest\tXrays\t( e.g.\t`…/left/MCUCXR_#####_0/1.png` or\t`…/right/MCUCXR_#####_0/1.png`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519a98f",
   "metadata": {},
   "source": [
    "# Preparing Data Lists\n",
    "\n",
    "We start by creating a list of data items. Each data item will be a dictionary of associated data: image filepath, label filepath, image origin, and presence of pathology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d18e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "# Adjust the paths here based on where you downloaded the data\n",
    "data_base_path = '/home/ebrahim/data/chest_xrays'\n",
    "montgomery_imgs_path = os.path.join(data_base_path, 'MontgomerySet/CXR_png')\n",
    "montgomery_segs_path_left = os.path.join(data_base_path, 'MontgomerySet/ManualMask/leftMask')\n",
    "montgomery_segs_path_right = os.path.join(data_base_path, 'MontgomerySet/ManualMask/rightMask')\n",
    "shenzhen_imgs_path = os.path.join(data_base_path, 'ChinaSet_AllFiles/CXR')\n",
    "shenzhen_segs_path = os.path.join(data_base_path, 'ChinaSet_AllFiles/CXR_segs')\n",
    "\n",
    "# We use glob to get lists of png filepaths\n",
    "montgomery_imgs = glob.glob(os.path.join(montgomery_imgs_path, '*.png'))\n",
    "montgomery_segs_left = glob.glob(os.path.join(montgomery_segs_path_left, '*.png'))\n",
    "montgomery_segs_right = glob.glob(os.path.join(montgomery_segs_path_right, '*.png'))\n",
    "\n",
    "shenzhen_imgs = glob.glob(os.path.join(shenzhen_imgs_path, '*.png'))\n",
    "shenzhen_segs = glob.glob(os.path.join(shenzhen_segs_path, '*.png'))\n",
    "\n",
    "# Here we map filepaths to image IDs; this will allow us to associate images\n",
    "# to their corresponding labels.\n",
    "file_path_to_ID = lambda p : os.path.basename(p)[7:11]\n",
    "montgomery_img_ids = list(map(file_path_to_ID,montgomery_imgs))\n",
    "montgomery_seg_ids_left = list(map(file_path_to_ID,montgomery_segs_left))\n",
    "montgomery_seg_ids_right = list(map(file_path_to_ID,montgomery_segs_right))\n",
    "shenzhen_img_ids = list(map(file_path_to_ID,shenzhen_imgs))\n",
    "shenzhen_seg_ids = list(map(file_path_to_ID,shenzhen_segs))\n",
    "\n",
    "# This function uses filename to extract whether tuberculosis is present.\n",
    "# While we are not necessarily interested in classifying images for tuberculosis here,\n",
    "# we still want to track which images have it so that our selection of validation data\n",
    "# can be made representative of the total population.\n",
    "file_path_to_abnormality = lambda p : bool(int(os.path.basename(p)[12]))\n",
    "\n",
    "# Finally, we define a list of data items.\n",
    "# This will be the input into the MONAI training and inference pipeline.\n",
    "# Each data item is a dictionary containing some associated data-- in this case\n",
    "# filepaths pointing to images and to associated segmentations, as well as a boolean\n",
    "# parameter indicating the presence of tuberculosis.\n",
    "data = []\n",
    "for img in montgomery_imgs: \n",
    "    img_id = file_path_to_ID(img)\n",
    "    seg_left = montgomery_segs_left[montgomery_seg_ids_left.index(img_id)]\n",
    "    seg_right = montgomery_segs_right[montgomery_seg_ids_right.index(img_id)]\n",
    "    tuberculosis = file_path_to_abnormality(img)\n",
    "    data.append({\n",
    "        'image' : img,\n",
    "        'mo_seg_left' : seg_left, # mo for montgomery\n",
    "        'mo_seg_right' : seg_right,\n",
    "        'tuberculosis' : tuberculosis,\n",
    "        'id' : 'montgomery:'+img_id,\n",
    "        'source' : \"montgomery\"\n",
    "    })\n",
    "skipped_no_seg = 0\n",
    "skipped_bad = 0\n",
    "for img in shenzhen_imgs:\n",
    "    img_id = file_path_to_ID(img)\n",
    "    if img_id not in shenzhen_seg_ids:\n",
    "        skipped_no_seg += 1\n",
    "        continue\n",
    "    seg = shenzhen_segs[shenzhen_seg_ids.index(img_id)]\n",
    "    tuberculosis = file_path_to_abnormality(img)\n",
    "    data.append({\n",
    "        'image' : img,\n",
    "        'sh_seg' : seg, # sh for shenzhen\n",
    "        'tuberculosis' : tuberculosis,\n",
    "        'id' : 'shenzhen:'+img_id,\n",
    "        'source' : \"shenzhen\"\n",
    "    })\n",
    "if skipped_no_seg>0:\n",
    "    print(f\"{skipped_no_seg} of the shenzhen images do not have an associated segmentation, and they were skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9face75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from segmentation_post_processing import SegmentationPostProcessing\n",
    "from segmentation_model_lib import * # This contains some custom monai transforms\n",
    "\n",
    "# Fixing the random seed is useful for making results reproducible\n",
    "monai.utils.misc.set_determinism(seed=9274)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef5628",
   "metadata": {},
   "source": [
    "When we select out validation data, we take care to select a subset of images that is representative of the total population. We do this by passing into the `classes` parameter of `partition_dataset_classes` a list of class names constructed by concatenating the parameters for which we care about having proportional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4899a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_valid = monai.data.utils.partition_dataset_classes(\n",
    "    data,\n",
    "    classes = list(map(lambda d : (d['tuberculosis'],d['source']), data)),\n",
    "    ratios = (8,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38fdead",
   "metadata": {},
   "source": [
    "# Creating Transforms\n",
    "\n",
    "MONAI has a large and powerful collection of transforms to draw upon. We use many of them here, and we also use some custom made transforms, which can be created easily by inheriting MONAI's transform classes.\n",
    "\n",
    "\n",
    "*Randomizable transforms* are the non-deterministic ones that can result in a different output each time they are given the same input. One thing to keep in mind is that any randomizable transform in a chain of transforms will interrupt caching, so all randomizable transforms should be put towards the end of the transform chain to the extent possible.\n",
    "\n",
    "Notice that the transforms used below have a 'D' in their name. This makes them *MapTransforms*, which means that they expect for data items to be *dictionaries*, and the transforms will operate on some of the *values* of those dictionaries. This is in contrast to a regular *Transform*, which would operate on data items directly. For a MapTransform, we must specify the keys for which the transform should operate on associated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22032a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "keys_to_delete = ['mo_seg_left', 'mo_seg_right', 'sh_seg']\n",
    "keys_to_delete += [k+\"_meta_dict\" for k in keys_to_delete] + [k+\"_transforms\" for k in keys_to_delete]\n",
    "\n",
    "# Base transform chain that is common to training and validation\n",
    "load_and_union_masks = [\n",
    "    monai.transforms.LoadImageD(reader='itkreader',keys = ['image']), # A few shenzhen images get mysteriously value-inverted with readers other than itkreader\n",
    "    monai.transforms.LambdaD(keys=['image'], func = rgb_to_grayscale), # A few of the shenzhen imgs are randomly RGB encoded rather than grayscale colormap\n",
    "    monai.transforms.LoadImageD(keys = ['mo_seg_left', 'mo_seg_right', 'sh_seg'], dtype=\"int8\", allow_missing_keys=True),\n",
    "    monai.transforms.TransposeD(keys = ['image', 'mo_seg_left', 'mo_seg_right', 'sh_seg'], indices = (1,0), allow_missing_keys=True),\n",
    "    monai.transforms.AddChannelD(keys = ['image']),\n",
    "    UnionMasksD(keys = ['mo_seg_left', 'mo_seg_right'], keyList=['mo_seg_left', 'mo_seg_right'], newKeyName='label'),\n",
    "    UnionMasksD(keys = ['sh_seg',], keyList=['sh_seg'], newKeyName='label'), # using for one-hot conversion, not \"union\"\n",
    "    monai.transforms.DeleteItemsD(keys = keys_to_delete),\n",
    "    monai.transforms.ToTensorD(keys = ['image', 'label'])\n",
    "]\n",
    "\n",
    "# Transform for validation\n",
    "transform_valid = monai.transforms.Compose([\n",
    "    *load_and_union_masks,\n",
    "    monai.transforms.ResizeD( # This resize\n",
    "        keys = ['image', 'label'],\n",
    "        spatial_size=(image_size,image_size),\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        align_corners = [False, None]\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Transform for training\n",
    "transform_train = monai.transforms.Compose([\n",
    "    *load_and_union_masks,\n",
    "    monai.transforms.ResizeD( # Standardize image size (e.g. good to do this before the spatial crop, for consistency)\n",
    "        keys = ['image', 'label'],\n",
    "        spatial_size=(512,512),\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        align_corners = [False, None]\n",
    "    ),\n",
    "    monai.transforms.RandZoomD(\n",
    "        keys = ['image', 'label'],\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        align_corners = [False, None],\n",
    "        prob=1.,\n",
    "        padding_mode=\"constant\",\n",
    "        min_zoom = 0.7,\n",
    "        max_zoom=1.3,\n",
    "    ),\n",
    "    monai.transforms.RandRotateD(\n",
    "        keys = ['image', 'label'],\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        align_corners = [False, None],\n",
    "        prob=1.,\n",
    "        range_x = np.pi/8,\n",
    "        padding_mode=\"zeros\",\n",
    "    ),\n",
    "    monai.transforms.RandGaussianSmoothD(\n",
    "        keys = ['image'],\n",
    "        prob = 0.4\n",
    "    ),\n",
    "    monai.transforms.RandAdjustContrastD(\n",
    "        keys = ['image'],\n",
    "        prob=0.4,\n",
    "    ),\n",
    "    monai.transforms.RandSpatialCropD(\n",
    "        keys = ['image', 'label'],\n",
    "        roi_size = 64, # used as minimum roi_size, since we enable random_size. max_roi_size is image size.\n",
    "        random_size = True\n",
    "    ),\n",
    "    monai.transforms.ResizeD( # Resize to desired final output size\n",
    "        keys = ['image', 'label'],\n",
    "        spatial_size=(image_size,image_size),\n",
    "        mode = ['bilinear', 'nearest'],\n",
    "        align_corners = [False, None]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003fe28",
   "metadata": {},
   "source": [
    "# PersistentDatasets\n",
    "\n",
    "We take our lists of data items and our transforms and we make datasets out of them. *Persistent*Datasets will pre-run their transforms on some number of data items, for faster access of transformed data later on. Unlike *Cache*Datasets, PersistentDatasets will do their caching on disk rather than in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = './cache'\n",
    "dataset_train = monai.data.PersistentDataset(data_train, transform_train, cache_dir)\n",
    "dataset_valid = monai.data.PersistentDataset(data_valid, transform_valid, cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a917619",
   "metadata": {},
   "source": [
    "# Previewing\n",
    "\n",
    "Here we preview some training images. If you just created `cache_dir` above and you look inside it then it should start out empty. After doing some previewing below, you will see `cache_dir` fill up as we query the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da01829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(data_item, show_bdry = False, overlay_seg = True, figsize = (7,7)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    im = data_item['image'].expand((3,)+data_item['image'].shape[1:])\n",
    "    im = im/im.max()\n",
    "    seg = data_item['label'].float()\n",
    "    if overlay_seg:\n",
    "        im[1,:,:] *= 1-0.3*seg[1,:,:]\n",
    "    if show_bdry:\n",
    "        seg_bdry = bdry(seg[1])\n",
    "        mask = (seg_bdry == 1.)\n",
    "        im[0,mask], im[1,mask], im[2,mask] = 1,0,0 # R, G, B\n",
    "    im = np.transpose(im,axes=(1,2,0))\n",
    "    plt.imshow(im, cmap='bone')\n",
    "    plt.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae5319",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.choice(range(len(dataset_train)))\n",
    "d = dataset_train[i]\n",
    "preview(d, show_bdry=False, overlay_seg=False, figsize=(12,12))\n",
    "d['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fa90b",
   "metadata": {},
   "source": [
    "# Define the Segmentation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cc865",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatial_dims = 2;\n",
    "image_channels = 1;\n",
    "seg_channels = 2; # lung, background\n",
    "seg_net_channel_seq = (16, 32, 64, 128, 256)\n",
    "stride_seq = (2,2,2,2) \n",
    "dropout_seg_net = 0.5\n",
    "num_res_units = 2\n",
    "\n",
    "seg_net = monai.networks.nets.UNet(\n",
    "    spatial_dims = spatial_dims,\n",
    "    in_channels = image_channels,\n",
    "    out_channels = seg_channels, \n",
    "    channels = seg_net_channel_seq,\n",
    "    strides = stride_seq,\n",
    "    dropout = dropout_seg_net,\n",
    "    num_res_units = num_res_units\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in seg_net.parameters())\n",
    "print(f\"seg_net has {num_params} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9f0e9",
   "metadata": {},
   "source": [
    "# Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_loss = monai.losses.DiceLoss(\n",
    "    to_onehot_y = False, # The segmentations we pass in are already in one-hot form\n",
    "    softmax = True, # Note that our segmentation network is missing the softmax at the end\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8553e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test drive\n",
    "data_item = dataset_train[42]\n",
    "seg_pred = seg_net(data_item['image'].unsqueeze(0)) # shape is (1,2,1024,1024), which is (B,N,H,W)\n",
    "\n",
    "dice_loss(\n",
    "    seg_net(data_item['image'].unsqueeze(0)), # input, one-hot\n",
    "    data_item['label'].unsqueeze(0), # target, one-hot\n",
    ").detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d282d12",
   "metadata": {},
   "source": [
    "# Defining a metric\n",
    "\n",
    "When evaluating a model, [metrics](https://docs.monai.io/en/stable/metrics.html) provide a little more functionality than plain functions or loss functions. Here we will use the [mean dice](https://docs.monai.io/en/stable/metrics.html#mean-dice) metric, which is\n",
    "- an [IterationMetric](https://docs.monai.io/en/stable/metrics.html#iterationmetric): instead of taking just raw model outputs, it can also operate on a _list_ of tensors\n",
    "- a [CumulativeMetric](https://docs.monai.io/en/stable/metrics.html#cumulative): gradually append/extend data into it and then aggregate when ready to do so.\n",
    "\n",
    "The real benefit of using a CumulativeMetric is the ability to compute the metric in a distributed manner across a large dataset. For more information on that, read about [distributed processes in pytorch](https://pytorch.org/tutorials/beginner/dist_overview.html), and then check out [this MONAI example](https://github.com/Project-MONAI/tutorials/blob/master/modules/compute_metric.py).\n",
    "\n",
    "We aren't doing any distributed computing in this notebook, but we will use the mean dice metric during validation just for the sake of demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_metric = monai.metrics.DiceMetric()\n",
    "\n",
    "# In order to use dice_metric, we need to discretize the output of segnet and express it in one-hot form\n",
    "# The following transform is convenient for this. It will turn logits of shape (num_seg_classes,H,W)\n",
    "# to discrete one-hot labels of the same shape (num_seg_classes,H,W)\n",
    "logits_to_discrete_one_hot = monai.transforms.AsDiscrete(argmax=True, to_onehot=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a401c0",
   "metadata": {},
   "source": [
    "# Previewing Segmentation Network Outputs\n",
    "\n",
    "Here we define a convenience function for previewing model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddaac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mask = lambda x : (x!=0).astype('float')\n",
    "bdry = lambda s : binary_mask((np.abs(np.diff(s, axis=0, prepend=0)) + np.abs(np.diff(s, axis=1, prepend=0)))!=0)\n",
    "\n",
    "def preview_seg_net(data_item, figsize=(15,10), print_score = True, show_heatmap = False, show_bdry=False, show_post_processing=0):\n",
    "    \"\"\"\n",
    "    Preview seg net prediciton\n",
    "    \n",
    "    Args:\n",
    "        data_item: A data item to input into seg_net.\n",
    "        figsize: figure size to be used at each matplotlib plotting call\n",
    "        print_score: show Dice score\n",
    "        show_heatmap: whether to show class probability image\n",
    "        show_bdry: whether to draw the boundry\n",
    "        show_post_processing: 0 to not show it,\n",
    "            1 to show post processed result,\n",
    "            2 to show post processed result and intermediate steps\n",
    "    \"\"\"\n",
    "    \n",
    "    seg_net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        im_device = data_item['image'].to(next(seg_net.parameters()).device.type)\n",
    "        seg_pred = seg_net(im_device.unsqueeze(0))[0].cpu()\n",
    "        _, max_indices = seg_pred.max(dim=0)\n",
    "        seg_pred_mask = (max_indices==1).type(torch.uint8)\n",
    "\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "        im = data_item['image'].expand((3,)+data_item['image'].shape[1:])\n",
    "        im = im/im.max()\n",
    "\n",
    "        seg_true = data_item['label'].float()\n",
    "        im_true = im.clone()\n",
    "        im_true[1,:,:] *= 1-0.4*seg_true[1,:,:]\n",
    "        if show_bdry:\n",
    "            seg_true_bdry = bdry(seg_true[1])\n",
    "            mask = (seg_true_bdry == 1.)\n",
    "            im_true[0,mask], im_true[1,mask], im_true[2,mask] = 1,0,0 # R, G, B\n",
    "        im_true = np.transpose(im_true,axes=(1,2,0))\n",
    "        ax1.imshow(im_true, cmap='bone')\n",
    "        ax1.set_title(\"true seg overlay\")\n",
    "        ax1.axis('off')\n",
    "\n",
    "        ax2.imshow(max_indices)\n",
    "        ax2.set_title(\"predicted seg\")\n",
    "        ax2.axis('off')\n",
    "\n",
    "        im_pred = im.clone()\n",
    "        im_pred[1,:,:] *= 1-0.4*seg_pred_mask\n",
    "        if show_bdry:\n",
    "            seg_pred_bdry = bdry(seg_pred_mask)\n",
    "            mask = (seg_pred_bdry == 1.)\n",
    "            im_pred[0,mask], im_pred[1,mask], im_pred[2,mask] = 1,0,0 # R, G, B\n",
    "        im_pred = np.transpose(im_pred,axes=(1,2,0))\n",
    "        ax3.imshow(im_pred, cmap='bone')\n",
    "        ax3.set_title(\"predicted seg overlay\")\n",
    "        ax3.axis('off')\n",
    "\n",
    "        plt.show();\n",
    "        \n",
    "        if show_heatmap:\n",
    "            f, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "            ax1.imshow(seg_pred.softmax(dim=0)[1])\n",
    "            ax1.axis('off')\n",
    "            print(\"predicted seg class probability maps:\")\n",
    "            plt.show()\n",
    "        \n",
    "        if show_post_processing!=0:\n",
    "            plt.figure(figsize = figsize)\n",
    "            seg_post_process = SegmentationPostProcessing()\n",
    "            seg_pred_processed = seg_post_process(seg_pred_mask)\n",
    "            im_pred = im.clone()\n",
    "            im_pred[1,:,:] *= 1-0.4*(seg_pred_processed==1)\n",
    "            im_pred[0,:,:] *= 1-0.4*(seg_pred_processed==2)\n",
    "            if show_bdry:\n",
    "                seg_pred_bdry1 = bdry(seg_pred_processed==1)\n",
    "                seg_pred_bdry2 = bdry(seg_pred_processed==2)\n",
    "                mask1 = (seg_pred_bdry1 == 1.)\n",
    "                mask2 = (seg_pred_bdry2 == 1.)\n",
    "                im_pred[0,mask1], im_pred[1,mask1], im_pred[2,mask1] = 1,0,0 # R, G, B\n",
    "                im_pred[0,mask2], im_pred[1,mask2], im_pred[2,mask2] = 0,1,0 # R, G, B\n",
    "            im_pred = np.transpose(im_pred,axes=(1,2,0))\n",
    "            plt.imshow(im_pred, cmap='bone')\n",
    "            plt.title(\"post-processed segmentation overlay\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            if show_post_processing>1:\n",
    "                seg_post_process.preview_intermediate_steps()\n",
    "\n",
    "        if print_score:\n",
    "            dice_metric.reset()\n",
    "            dice_metric(\n",
    "                logits_to_discrete_one_hot(seg_pred).unsqueeze(0),\n",
    "                data_item['label'].unsqueeze(0),\n",
    "            )\n",
    "            score = dice_metric.aggregate()\n",
    "            print(f\"Dice metric: {(1.-score.item()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9e9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = monai.transforms.AsDiscrete(argmax=True, to_onehot=2)(seg_pred[0]).unsqueeze(0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c3c02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try seg_net on a random image.\n",
    "preview_seg_net(random.choice(dataset_train), show_bdry=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005fce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a good sanity check. Ground truth label trnsors should have discrete values--\n",
    "# let's make sure that's still the case after all the transforms are applied:\n",
    "dataset_train[0]['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d7c51",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794629ff",
   "metadata": {},
   "source": [
    "Define *dataloaders*, which draw their data from datasets and collate it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0975d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = monai.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    collate_fn = list_data_collate_no_meta # (It's normally not necessary to define a custom collate_fn)\n",
    ")\n",
    "\n",
    "dataloader_valid = monai.data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=64,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    collate_fn = list_data_collate_no_meta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d8fe9",
   "metadata": {},
   "source": [
    "Do the initial setup for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "\n",
    "epoch_number = 0\n",
    "training_losses = [] \n",
    "validation_scores = []\n",
    "preview_index = random.choice(range(len(dataset_valid)))\n",
    "best_validation_score = float('-inf')\n",
    "best_validation_epoch = -1\n",
    "\n",
    "validate_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a5fed",
   "metadata": {},
   "source": [
    "Finally, the training loop! We save the model with the best validation score.\n",
    "\n",
    "If you've already done some training and run the \"`CHECKPOINT CELL; SAVE`\" cell [below](#Saving), then you can skip the training loop and uncomment the \"`CHECKPOINT CELL; LOAD`\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d920523",
   "metadata": {},
   "source": [
    "You can also skip this training loop and try the alternative ignite-based workflow appraoch at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57875d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "seg_net.to(device)\n",
    "\n",
    "# shift things to always validate on last epoch\n",
    "validate_this_epoch = lambda epoch_number : epoch_number%validate_every==(max_epochs-1)%validate_every\n",
    "\n",
    "while epoch_number < max_epochs:\n",
    "    \n",
    "    print(f\"Epoch {epoch_number+1}/{max_epochs} ...\")\n",
    "    \n",
    "    if validate_this_epoch(epoch_number):\n",
    "        preview_seg_net(dataset_valid[preview_index], figsize=(6,6), print_score=False);\n",
    "    \n",
    "    seg_net.train()\n",
    "    losses = []\n",
    "    for batch in dataloader_train:\n",
    "        imgs = batch['image'].to(device)\n",
    "        true_segs = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predicted_segs = seg_net(imgs)\n",
    "        loss = dice_loss(predicted_segs, true_segs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    training_loss = np.mean(losses)\n",
    "    training_losses.append([epoch_number, training_loss])\n",
    "    \n",
    "    print(f\"\\ttraining loss: {training_loss}\")\n",
    "\n",
    "    if validate_this_epoch(epoch_number):\n",
    "    \n",
    "        seg_net.eval()\n",
    "        dice_metric.reset() # We will aggregate the dice metric\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader_valid:\n",
    "                imgs = batch['image'].to(device)\n",
    "                true_segs = batch['label'].to(device)\n",
    "                predicted_segs = seg_net(imgs)\n",
    "                dice_metric(\n",
    "                    [logits_to_discrete_one_hot(predicted_seg) \n",
    "                        for predicted_seg in monai.data.decollate_batch(predicted_segs)],\n",
    "                    true_segs\n",
    "                )\n",
    "            validation_score = dice_metric.aggregate().item()\n",
    "\n",
    "        print(f\"\\tvalidation mean dice score: {validation_score}\")\n",
    "        \n",
    "        validation_scores.append([epoch_number, validation_score])\n",
    "        \n",
    "        if validation_score > best_validation_score:\n",
    "            best_validation_score = validation_score\n",
    "            torch.save(seg_net.state_dict(),f'seg_net_bestval.pth')\n",
    "            best_validation_epoch = epoch_number\n",
    "    \n",
    "    epoch_number +=1\n",
    "\n",
    "del imgs, true_segs, predicted_segs, loss\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "seg_net.load_state_dict(torch.load(f'seg_net_bestval.pth'))\n",
    "print(f\"Loaded model state during the best validation score, which was during epoch {best_validation_epoch}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c93d0",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; LOAD\n",
    "\n",
    "# Uncomment the lines below to load a previous post-training state.\n",
    "\n",
    "# run_id_load = '0023'\n",
    "# run_id = None\n",
    "# load_path = f'model{run_id_load}.pth'\n",
    "\n",
    "# model_dict = torch.load(load_path)\n",
    "\n",
    "# seg_net = model_dict['model']\n",
    "# learning_rate = model_dict['learning_rate']\n",
    "# optimizer = torch.optim.Adam(seg_net.parameters(), learning_rate)\n",
    "# optimizer.load_state_dict(model_dict['optimizer_state_dict'])\n",
    "# training_losses = model_dict['training_losses']\n",
    "# validation_scores = model_dict['validation_scores']\n",
    "# epoch_number = model_dict['epoch_number']\n",
    "# best_validation_score = model_dict['best_validation_score']\n",
    "# best_validation_epoch = model_dict['best_validation_epoch']\n",
    "# image_size = model_dict['image_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c3c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = '0029' # Set a new run ID each time\n",
    "save_path = f'model{run_id}.pth'\n",
    "if (os.path.exists(save_path)):\n",
    "    del run_id, save_path\n",
    "    raise Exception(\"Please change run_id so you don't overwrite things.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT CELL; SAVE\n",
    "\n",
    "torch.save(\n",
    "    {\n",
    "        'model': seg_net.cpu(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'learning_rate': learning_rate,\n",
    "        'training_losses': training_losses,\n",
    "        'validation_scores': validation_scores,\n",
    "        'epoch_number': epoch_number,\n",
    "        'best_validation_score': best_validation_score,\n",
    "        'best_validation_epoch': best_validation_epoch,\n",
    "        'image_size': image_size,\n",
    "    }, \n",
    "    save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b3979b",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_against_epoch_numbers(epoch_value_pairs, label):\n",
    "    array = np.array(epoch_value_pairs)\n",
    "    plt.plot(array[:,0], array[:,1], label=label)\n",
    "\n",
    "plot_against_epoch_numbers(training_losses, label=\"training\")\n",
    "plot_against_epoch_numbers(validation_scores, label=\"validation\")\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('dice loss')\n",
    "plt.title('seg net training')\n",
    "if run_id is not None: plt.savefig(f'seg_net_losses{run_id}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31549d",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Run the cell below to try the segmentation model on random validation images\n",
    "\n",
    "We also show some post processing done separately using ITK. It ensures that we have two contiguous lung reigions with no holes, and it separates them into left and right lungs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c948d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Try on a random validation image\n",
    "data_item_index = random.choice(range(len(dataset_valid)))\n",
    "print(data_item_index, data_item['id'])\n",
    "data_item = dataset_valid[data_item_index]\n",
    "with torch.no_grad():\n",
    "    preview_seg_net(data_item, show_heatmap=False, show_bdry=True, show_post_processing=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation, here is our final dice score!\n",
    "print(best_validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b71dc",
   "metadata": {},
   "source": [
    "##  Sliding Window Inference\n",
    "\n",
    "*Sliding window inference* is a way to run our network on smaller pieces of an image, and then glue the outputs together into a full result. This might be preferred if there are memory constraints that prevent operating on an image all at once, which often happens when a model is deployed to a variety of environments that don't have the same computational resources that were available during training.\n",
    "\n",
    "There are a couple of ways to do sliding window inference\n",
    "1. There's the general concept of an [inferer](https://docs.monai.io/en/stable/inferers.html) in MONAI. An inferer represents the code that takes a pytorch module (like `seg_net`) with an input and yields a final output. For custom applications, a typical approach would be to derive from `Inferer` and implement your own `__call__`. There are also a few pre-made inferers, one of which is [SlidingWindowInferer](https://docs.monai.io/en/stable/inferers.html#slidingwindowinferer).\n",
    "1. One can skip the whole inferer concept and directly use the function `monai.inferers.sliding_window_inference`. This function is internally used by `monai.inferers.SlidingWindowInferer` anyway, so it's the same thing in the end. To see an example of this, check out the [spleen segmentation tutorial](https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/spleen_segmentation_3d.ipynb).\n",
    "\n",
    "We'll do (1) here. It's completely overkill in this example-- `seg_net` does not have high memory demands to operate on a full image-- but let's just see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5831a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with the roi_size, which is the sliding window size, and run the cell below to see the effect\n",
    "# Use multiples of 32, to stick to the expected number of strided convolutions\n",
    "inferer = monai.inferers.SlidingWindowInferer(roi_size = 96, mode='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_index = random.choice(range(len(dataset_valid)))\n",
    "print(data_item_index, data_item['id'])\n",
    "data_item = dataset_valid[data_item_index]\n",
    "\n",
    "seg_net.eval()\n",
    "img = data_item['image'].to(next(seg_net.parameters()).device.type)\n",
    "\n",
    "with torch.no_grad():\n",
    "    seg_output = inferer(img.unsqueeze(0),seg_net).cpu()\n",
    "\n",
    "seg_mask = seg_output.argmax(dim=1)[0]\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,10))\n",
    "\n",
    "im = img.expand((3,)+img.shape[1:])\n",
    "im = im/im.max()\n",
    "\n",
    "seg_true = data_item['label'].float()\n",
    "im_true = im.cpu()/im.max()\n",
    "im_true[1,:,:] *= 1-0.4*seg_true[1,:,:]\n",
    "im_true = np.transpose(im_true,axes=(1,2,0))\n",
    "ax1.imshow(im_true, cmap='bone')\n",
    "ax1.set_title(\"true seg overlay\")\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(seg_mask)\n",
    "ax2.set_title(\"predicted seg\")\n",
    "ax2.axis('off')\n",
    "\n",
    "im_pred = im.cpu()/im.max()\n",
    "im_pred[1,:,:] *= 1-0.4*seg_mask\n",
    "im_pred = np.transpose(im_pred,axes=(1,2,0))\n",
    "ax3.imshow(im_pred, cmap='bone')\n",
    "ax3.set_title(\"predicted seg overlay\")\n",
    "ax3.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "seg_post_process = SegmentationPostProcessing()\n",
    "seg_pred_processed = seg_post_process(seg_mask)\n",
    "im_pred = im.clone()\n",
    "im_pred[1,:,:] *= 1-0.4*(seg_pred_processed==1)\n",
    "im_pred[0,:,:] *= 1-0.4*(seg_pred_processed==2)\n",
    "im_pred = np.transpose(im_pred,axes=(1,2,0))\n",
    "plt.imshow(im_pred, cmap='bone')\n",
    "plt.title(\"post-processed segmentation overlay\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799f07e",
   "metadata": {},
   "source": [
    "You may notice that the network struggles when the sliding window size becomes too small. Some of this can be alleviated by adjusting the `RandSpatialCropD` transform that is used during training, but that can only go so far. It's not surprising that the network is able to peform better when it has more global context to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c5e90",
   "metadata": {},
   "source": [
    "# Engines appraoch to training\n",
    "\n",
    "[Engines](https://docs.monai.io/en/stable/engines.html) and [event handlers](https://docs.monai.io/en/stable/handlers.html) are MONAI's window into the [PyTorch Ignite](https://pytorch.org/ignite/index.html) way of doing things. Instead of explicitly defining a `for` loop to do our training, we construct an ignite _engine_ called `trainer` below. Calling `trainer.run()` actually executes the training process. In this case the trainer is a MONAI `SupervisedTrainer`, which has a lot of sensible defaults for \"image and label\" style training.\n",
    "\n",
    "Normally when using Ignite we define event handler callables and associate those callables to various events on our Ignite engine. In the MONAI `SupervisedTrainer`, we pass the handlers into a parameter `train_handlers`. What we actually have to pass is a list of objects that have an `attach` method, which is what actually associates callables to events. In the cell below we create some objects that have an `attach` method. These objects will do the same sort of work that we were previously doing inside an explicit `for` loop.\n",
    "\n",
    "Instead of creating custom handlers, one could also look to the [large collection of ready-to-use handlers provided by MONAI](https://docs.monai.io/en/stable/handlers.html). The reader is encouraged to browse through this collection, but for now I find it easier to see what's going on by making our own event handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining event handlers\n",
    "\n",
    "from ignite.engine import Events\n",
    "\n",
    "# An event handler that keeps track of average training losses\n",
    "class TrainingLossCollector:\n",
    "    def __init__(self):\n",
    "        self.losses = [] # iteration losses for the current epoch\n",
    "        self.training_losses = [] # overall running list of losses by epoch, each averaged over a full epoch\n",
    "    def attach(self, engine):\n",
    "        engine.add_event_handler(monai.engines.utils.IterationEvents.LOSS_COMPLETED, self.addIterationLoss)\n",
    "        engine.add_event_handler(Events.EPOCH_STARTED, self.onEpochStart)\n",
    "        engine.add_event_handler(Events.EPOCH_COMPLETED, self.onEpochEnd)\n",
    "    def addIterationLoss(self, engine):\n",
    "        self.losses.append(engine.state.output['loss'].item())\n",
    "    def onEpochStart(self, engine):\n",
    "        self.losses=[]\n",
    "    def onEpochEnd(self, engine):\n",
    "        epoch_mean_loss = np.mean(self.losses)\n",
    "        self.training_losses.append([engine.state.epoch, epoch_mean_loss])\n",
    "        print(f\"Epoch {engine.state.epoch} training loss: {epoch_mean_loss}\")\n",
    "\n",
    "# An event handler that runs validation, keeps track of validation losses, and keeps track of the best model.\n",
    "# Instead of writing a custom handler here, we could have constructed a monai.engines.SupervisedEvaluator\n",
    "# and then created a validation handler automatically by using monai.handlers.ValidationHandler.\n",
    "# To get the exact functionality I wanted, I found it more straight-forward to just write the handler.\n",
    "class CustomValidationHandler:\n",
    "    \n",
    "    best_segnet_filename = 'CustomValidationHandler_seg_net_bestval.pth'\n",
    "    \n",
    "    def __init__(self, seg_net, dice_metric, dataloader_valid, device):\n",
    "        self.seg_net = seg_net\n",
    "        self.dice_metric = dice_metric\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.device = device\n",
    "        self.best_validation_score = float('-inf')\n",
    "        self.best_validation_epoch = -1\n",
    "        self.validation_scores = []\n",
    "    \n",
    "    def attach(self, engine):\n",
    "        engine.add_event_handler(\n",
    "            Events.EPOCH_COMPLETED(every=validate_every) | Events.COMPLETED | Events.STARTED,\n",
    "            self.validate\n",
    "        )\n",
    "        engine.add_event_handler(\n",
    "            Events.COMPLETED,\n",
    "            self.load_best_seg_net\n",
    "        )\n",
    "        \n",
    "    def validate(self, engine):\n",
    "        self.seg_net.eval()\n",
    "        self.dice_metric.reset()\n",
    "        with torch.no_grad():\n",
    "            for batch in self.dataloader_valid:\n",
    "                imgs = batch['image'].to(self.device)\n",
    "                true_segs = batch['label'].to(self.device)\n",
    "                predicted_segs = self.seg_net(imgs)\n",
    "                self.dice_metric(\n",
    "                    [logits_to_discrete_one_hot(predicted_seg) \n",
    "                        for predicted_seg in monai.data.decollate_batch(predicted_segs)],\n",
    "                    true_segs\n",
    "                )\n",
    "            validation_score = dice_metric.aggregate().item()\n",
    "\n",
    "        print(f\"Epoch {engine.state.epoch} validation mean dice score: {validation_score}\")\n",
    "        \n",
    "        self.validation_scores.append([engine.state.epoch, validation_score])\n",
    "        \n",
    "        if validation_score > self.best_validation_score:\n",
    "            self.best_validation_score = validation_score\n",
    "            torch.save(self.seg_net.state_dict(), self.best_segnet_filename)\n",
    "            self.best_validation_epoch = engine.state.epoch\n",
    "    \n",
    "    def load_best_seg_net(self, engine):\n",
    "        if os.path.exists(self.best_segnet_filename):\n",
    "            self.seg_net.load_state_dict(torch.load(self.best_segnet_filename))\n",
    "            print(f\"Loaded model state during the best validation score, which was during epoch {self.best_validation_epoch}.\")\n",
    "\n",
    "# An event handler that shows a preview of a specific data item every few epochs,\n",
    "# so we have something nice to look at while watching the training take place.\n",
    "class ImageDisplayHandler:\n",
    "    def __init__(self, data_item, preview_every):\n",
    "        \"\"\"Preview a specific data item every preview_every epochs.\"\"\"\n",
    "        self.data_item = data_item\n",
    "        self.preview_every = preview_every\n",
    "    def attach(self, engine):\n",
    "        engine.add_event_handler(Events.EPOCH_COMPLETED(every=self.preview_every), self)\n",
    "    def __call__(self, engine):\n",
    "        preview_seg_net(self.data_item, figsize=(6,6), print_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the handler objects\n",
    "\n",
    "validation_handler = CustomValidationHandler(seg_net, dice_metric, dataloader_valid, device)\n",
    "image_display_handler = ImageDisplayHandler(dataset_valid[preview_index], 5)\n",
    "training_loss_collector = TrainingLossCollector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17490edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training engine\n",
    "\n",
    "seg_net.to(device)\n",
    "\n",
    "trainer = monai.engines.SupervisedTrainer(\n",
    "    device = device,\n",
    "    max_epochs = 10,\n",
    "    train_data_loader = dataloader_train,\n",
    "    network = seg_net,\n",
    "    optimizer = optimizer,\n",
    "    loss_function = dice_loss,\n",
    "    train_handlers = [training_loss_collector, validation_handler, image_display_handler],\n",
    "    amp=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c7c5e",
   "metadata": {},
   "source": [
    "Notice that [automatic mixed precision](https://docs.monai.io/en/stable/highlights.html#auto-mixed-precision-amp)\n",
    "was enabled by simply setting `amp=True` in the engine parameters. This would have been more complicated with plain PyTorch (see [PyTorch docs on AMP](https://pytorch.org/docs/stable/notes/amp_examples.html), or [this MONAI tutorial](https://github.com/Project-MONAI/tutorials/blob/master/acceleration/automatic_mixed_precision.ipynb) that does AMP explicitly without using an engine)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf4304",
   "metadata": {},
   "source": [
    "Now we can start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9463b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162abb88",
   "metadata": {},
   "source": [
    "The following cell defines the variables needed to return to where you were [above](#Saving) in the notebook and skip the training \"for loop\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = training_loss_collector.training_losses\n",
    "validation_scores = validation_handler.validation_scores\n",
    "best_validation_score = validation_handler.best_validation_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
